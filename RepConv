def get_dwconv(dim, kernel, groups, bias):
    return nn.Conv2d(dim, dim, kernel_size=kernel, padding=(kernel-1)//2, bias=bias, groups=dim)

class Rep2conv(nn.Module):
    def __init__(self, inchannel, outchannel, kernel_size, groups, gflayer=None, h=14, w=8, s=1.0):
        super().__init__()
        self.dims = [inchannel // 2, inchannel]
        # self.dims.reverse()
        self.proj_in = nn.Conv2d(inchannel, 2 * inchannel, 1)
        #self.channel = inchannel/2
        self.proj1 = nn.Conv2d(self.dims[0], self.dims[1], 1)
        self.proj2 = nn.Conv2d(inchannel, 4*inchannel, 1)
        self.act = nn.GELU()
        self.proj_out = nn.Conv2d(4*inchannel, outchannel, 1)
        self.proj3 = nn.Conv2d(inchannel, outchannel, 1)

        if gflayer is None:
            self.dwconv = get_dwconv(inchannel, kernel_size, groups, bias=True)
        else:
            self.dwconv = gflayer(inchannel, h=h, w=w)

        self.scale = s
        print('[gnconv]', 2, 'order with dims=', self.dims, 'scale=%.4f' % self.scale)

    def forward(self, x, mask=None, dummy=False):
        B, C, H, W = x.shape

        fused_x = x
        #print("fused_x:", fused_x.shape)

        pwa, abc = torch.split(fused_x, (self.dims[0], self.dims[0]), dim=1)
        #print("pwa:{}  abc:{}".format(pwa.shape, abc.shape))

        dw_abc = self.dwconv(x)

        dw_list = torch.split(dw_abc, [self.dims[0], self.dims[0]], dim=1)
        x = pwa * dw_list[0]

        x = self.proj1(x)
        x = x * dw_abc
        x = self.proj2(x)
        x = self.act(x)
        x = self.proj3(x)

        return x

class Rep2Conv2D(Conv2D):
    def __init__(self, inc, ouc, kernel_size, g):
        super().__init__(inc, ouc, kernel_size, g)
        self.conv = Rep2conv(inc, ouc, kernel_size, 1)

    def __str__(self):
        return 'Rep2Conv'
